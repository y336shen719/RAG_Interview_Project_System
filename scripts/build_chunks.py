import os  # used to walk through files
import re  # used for regular expression
import json  # used to convert md into json
from pathlib import Path

# =============================
# Project root detection
# =============================
# Use file location to ensure stable path resolution
PROJECT_ROOT = Path(__file__).resolve().parent.parent

BASE_DIR = PROJECT_ROOT / "knowledge_base"
OUTPUT_FILE = PROJECT_ROOT / "chunks.json"

MIN_CHARS = 100  # min chars in a chunk, prevent too short & meaningless chunks


def read_markdown(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


# remove YAML frontmatter
def remove_frontmatter(text):
    """
    Remove YAML frontmatter (--- ... --- at top of file)
    Prevent metadata pollution in embedding.
    """
    if text.startswith("---"):
        parts = text.split("---", 2)
        if len(parts) >= 3:
            return parts[2].strip()
    return text


# for resume / project splitting
def split_by_headers(text):
    '''
split markdown text based on their headers hierarchy (# , ## , ### )
keep header to its content
    '''
    sections = re.split(
        r"(?=^#{1,6} )",  # support up to ######
        text,
        flags=re.MULTILINE
    )  # ?= :lookahead, keep header to its content

    return [
        s.strip()
        for s in sections
        if s.strip() and len(s.strip()) >= MIN_CHARS
    ]  # remove blank and too short chunks


# for interview Q&A
def split_interview_sections(text):
    '''
Split interview bank by ### headers.
Each ### section becomes one chunk.
Ensure no mixing with next section.
    '''
    raw_sections = re.split(
        r"(?=^### )",
        text,
        flags=re.MULTILINE
    )  # every Q&A starts with ###

    cleaned_sections = []

    for sec in raw_sections:
        sec = sec.strip()

        if not sec.startswith("###"):
            continue

        # cut off trailing section markers like:
        # ---
        # ## next section
        sec = re.split(r"\n---|\n## ", sec)[0].strip()

        if len(sec) >= MIN_CHARS:
            cleaned_sections.append(sec)

    return cleaned_sections


# build chunks based on text + metadata
def create_chunk(text, source_type, file_name, extra_meta=None):
    if len(text) < MIN_CHARS:
        return None

    metadata = {
        "source_type": source_type,
        "file_name": file_name,
    }

    if extra_meta:
        metadata.update(extra_meta)

    return {
        "content": text,
        "metadata": metadata
    }


# process md files
def process_file(file_path):
    text = read_markdown(file_path)

    # remove frontmatter first (critical)
    text = remove_frontmatter(text)

    file_name = file_path.name
    folder_name = file_path.parent.name

    chunks = []

    # Interview Bank
    if folder_name == "interview_bank":
        sections = split_interview_sections(text)

        for section in sections:
            header_match = re.match(r"^### (.+)", section)
            question_title = header_match.group(1).strip() if header_match else "unknown"

            chunk = create_chunk(
                section,
                source_type="interview",
                file_name=file_name,
                extra_meta={"question": question_title}
            )

            if chunk:
                chunks.append(chunk)

    # Projects
    elif folder_name == "projects":
        sections = split_by_headers(text)

        for section in sections:
            header_match = re.match(r"^(#{1,6}) (.+)", section)
            section_title = header_match.group(2).strip() if header_match else "unknown"

            chunk = create_chunk(
                section,
                source_type="project",
                file_name=file_name,
                extra_meta={"section": section_title}
            )

            if chunk:
                chunks.append(chunk)

    # Resume
    elif folder_name == "resume":
        sections = split_by_headers(text)

        for section in sections:
            header_match = re.match(r"^(#{1,6}) (.+)", section)
            section_title = header_match.group(2).strip() if header_match else "unknown"

            chunk = create_chunk(
                section,
                source_type="resume",
                file_name=file_name,
                extra_meta={"section": section_title}
            )

            if chunk:
                chunks.append(chunk)

    return chunks


# main code
def main():
    all_chunks = []

    # use pathlib rglob for cleaner traversal
    for file_path in BASE_DIR.rglob("*.md"):
        file_chunks = process_file(file_path)
        all_chunks.extend(file_chunks)  # use extend instead of append

    print("=" * 50)
    print(f"Total chunks created: {len(all_chunks)}")
    print("=" * 50)

    type_counts = {}
    for chunk in all_chunks:
        t = chunk["metadata"]["source_type"]
        type_counts[t] = type_counts.get(t, 0) + 1

    print("Chunk distribution:")
    for k, v in type_counts.items():
        print(f"  {k}: {v}")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, indent=2, ensure_ascii=False)

    print(f"\nSaved to {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
